{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Project 2\n",
    "# Written by: Harsh Deep Kour(40082906) and Iknoor Singh Arora(40082312)\n",
    "# For COMP 6721 Section: FK (1779) - Fall 2019\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(r'/Users/harshkour/Desktop/AI-P2/hn2018_2019-2.csv')\n",
    "data = pd.read_csv(r'hn2018_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of records in Training are: 90\n"
     ]
    }
   ],
   "source": [
    "traindata=data[(data['Created At'] > '2018-01-01') & (data['Created At'] < '2019-01-01')]\n",
    "traindatalen=len(traindata)\n",
    "print(\"No. of records in Training are: \" + str(traindatalen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of records in Testing are: 10\n"
     ]
    }
   ],
   "source": [
    "testdata=data[(data['Created At'] >= '2019-01-01')]\n",
    "print(\"No. of records in Testing are: \" + str(len(testdata)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post_Type_required = ['story','ask_hn','show_hn','poll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for Pos tag mapping \n",
    "def pos_mapping(word_tag):\n",
    "        \n",
    "    if word_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif word_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif word_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif word_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.ADV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Vocabulary is :475\n",
      "Number of Bigrams in Vocabulary are: 185\n",
      "No. of words removed are :10\n"
     ]
    }
   ],
   "source": [
    "#Creating vocabulary for training data\n",
    "lemmat = WordNetLemmatizer()\n",
    "\n",
    "vocablist=[]\n",
    "bigramslist = []\n",
    "unique_bigrams =[]\n",
    "nonvocablist = []\n",
    "nonvocab_unique = []\n",
    "#english_vocab = nltk.corpus.words.words()\n",
    "\n",
    "\n",
    "for i, row in traindata.iterrows():\n",
    "    final=' '\n",
    "    bigrams_final = []\n",
    "    unigram = []\n",
    "    nonunigram = []\n",
    "    vocab = []\n",
    "    pattern = '[\\w]+'\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    title = row.Title\n",
    "    title = title.replace(\"'\",\"\")\n",
    "    list_wordtokenized=tokenizer.tokenize(title)# tokenization using regextokenizer\n",
    "    posttag_list = pos_tag(list_wordtokenized)#finding pos tags\n",
    "    bigrams = list(nltk.bigrams(posttag_list))#libray for bigram creation\n",
    "    \n",
    "    #creating bygrams for pos_tags starting with N\n",
    "    for (w1,w2) in bigrams :\n",
    "        \n",
    "        flagw1 = False\n",
    "        flagw2 = False\n",
    "        flag_w1 = False\n",
    "        flag_w2 = False\n",
    "        posw1 = pos_mapping(w1[1])\n",
    "        posw2 = pos_mapping(w2[1])\n",
    "        lemword1=(lemmat.lemmatize(w1[0],posw1)).lower()\n",
    "        lemword2=(lemmat.lemmatize(w2[0],posw2)).lower()\n",
    "        \n",
    "        \n",
    "        if('_' in lemword1):\n",
    "            word1 = lemword1.replace(\"_\",\"\")\n",
    "            if word1.isalpha():\n",
    "                flag_w1 = True \n",
    "        if('_' in lemword2):\n",
    "            word2 = lemword2.replace(\"_\",\"\")\n",
    "            if word2.isalpha():\n",
    "                flag_w2 = True \n",
    "        #if (lemword1.isalpha() and len(lemword1)!=1) or flag_w1 or (len(lemword1)==1 and (lemword1 == 'a' or lemword1 == 'i')):\n",
    "        if (lemword1.isalpha()) or flag_w1:\n",
    "            flagw1 = True\n",
    "        else:\n",
    "            nonvocablist.append(lemword1) \n",
    "        #if (lemword2.isalpha() and len(lemword2)!=1) or flag_w2 or (len(lemword2)==1 and (lemword2 == 'a' or lemword2 == 'i')):\n",
    "        if (lemword2.isalpha()) or flag_w2:    \n",
    "            flagw2 = True\n",
    "        else:\n",
    "            nonvocablist.append(lemword2)\n",
    "            \n",
    "        if posw1 == 'n' and posw2 == 'n' and flagw1 and flagw2:\n",
    "            x = \"%s %s\" % (lemword1.strip(),lemword2.strip())\n",
    "            bigrams_final.append(x)\n",
    "            nonunigram.append(lemword1)\n",
    "            nonunigram.append(lemword2)\n",
    "        else:\n",
    "            if flagw1:\n",
    "                unigram.append(lemword1)\n",
    "            if flagw2:\n",
    "                unigram.append(lemword2)\n",
    "                \n",
    "    unigram_final = list(set(unigram) - set(nonunigram))\n",
    "    vocab.extend(unigram_final)\n",
    "    vocab.extend(bigrams_final)\n",
    "    vocablist.extend(vocab)\n",
    "    bigramslist.extend(bigrams_final)\n",
    "    \n",
    "    traindata.at[i, 'Title']= vocab\n",
    "\n",
    "unique_bigrams = np.unique(bigramslist)\n",
    "vocab_unique=np.unique(vocablist)\n",
    "vocab_unique=list(vocab_unique)\n",
    "vocab_unique.sort()\n",
    "nonvocab_unique=np.unique(nonvocablist)\n",
    "nonvocab_unique=list(nonvocab_unique)\n",
    "nonvocab_unique.sort()\n",
    "print(\"Length of Training Vocabulary is :\" + str(len(vocab_unique)))\n",
    "print(\"Number of Bigrams in Vocabulary are: \" + str(len(unique_bigrams)))\n",
    "print(\"No. of words removed are :\" + str(len(nonvocab_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a vocabulary file\n",
    "vocabfile = open('vocabulary.txt', \"w\",encoding=\"utf-8\")\n",
    "vocabfile.write(\"\\n\".join(str(item) for item in vocab_unique))\n",
    "vocabfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a remove words file \n",
    "nonvocabfile = open('remove_word.txt', \"w\",encoding=\"utf-8\")\n",
    "nonvocabfile.write(\"\\n\".join(str(item) for item in nonvocab_unique))\n",
    "nonvocabfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story\n",
      "30\n",
      "ask_hn\n",
      "20\n",
      "show_hn\n",
      "30\n",
      "poll\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "type_dict = {} # 2d dictionary for each word's frequency corresponding to class\n",
    "\n",
    "new_list = Post_Type_required\n",
    "prob_type_dict={} # dictionary for class score\n",
    "\n",
    "for type_value in new_list:\n",
    "    all_rows = traindata.loc[traindata['Post Type'] == type_value]\n",
    "    print(type_value)\n",
    "    print(len(all_rows))\n",
    "    prob_type_dict[type_value]=len(all_rows)/traindatalen\n",
    "    type_dict[type_value]=pd.Series(np.concatenate([x for x in all_rows['Title']])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating the model and adding probability values for each class for words in model text file.\n",
    "#It returns main dictionary and test dictionary which has prob and freq corresponding to each word.\n",
    "\n",
    "def model_building(file_name_writeread,file_name_write,smooth_factor):\n",
    "    file_text = open(file_name_writeread, \"w\", encoding=\"utf-8\")\n",
    "    main_dict = {} # 2d dictionary type - word - freq/classtype +probability/classtype\n",
    "    for post_type,val in type_dict.items():\n",
    "        sumval= sum(type_dict[post_type])\n",
    "        main_dict[post_type] = {}\n",
    "        word_dict = {}\n",
    "        for word in vocab_unique:\n",
    "            word_freq=0\n",
    "            if(word in type_dict[post_type]):     \n",
    "                word_freq=type_dict[post_type][word]\n",
    "\n",
    "            probab=(word_freq+smooth_factor)/(sumval+len(vocab_unique))\n",
    "            word_dict[word] = str(word_freq) + \"  \" + str(probab)\n",
    "            \n",
    "        main_dict[post_type] = word_dict\n",
    "\n",
    " \n",
    "    post_type_list = []\n",
    "    counter = 0\n",
    "    test_dict = {}\n",
    "    for word in vocab_unique:\n",
    "        prob_word = []\n",
    "        counter = counter + 1\n",
    "        freprobstr = \"\"\n",
    "        for keys in main_dict.keys():\n",
    "            #print(\"key is \" + str(keys))\n",
    "            w_dict = main_dict[keys]\n",
    "            freqprob = w_dict[word].split(\"  \")\n",
    "\n",
    "            prob_word.append(float(freqprob[1]))\n",
    "            if freprobstr == \"\":\n",
    "                freprobstr =  w_dict[word]\n",
    "            else:\n",
    "                freprobstr = \"  \" + freprobstr + \"  \" + w_dict[word]\n",
    "        test_dict[word] = prob_word\n",
    "        word = str(word).strip()   \n",
    "        file_text.write(str(counter) + \"  \" + word + \"  \" + freprobstr.strip() + \"\\r\\n\")  \n",
    "\n",
    "    #print(main_dict)\n",
    "    file_text.close()\n",
    "    return main_dict,test_dict              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating total probability of the words in the title for each class\n",
    "def calcvocabscore(main_dict,test_dict):\n",
    "    Score_dict = {}   \n",
    "    for word in test_dict:\n",
    "        final_classscores_list = []       \n",
    "        probclasslist =  test_dict[word]\n",
    "        word_log0= math.log(probclasslist[0],10)\n",
    "        word_log1= math.log(probclasslist[1],10)\n",
    "        word_log2= math.log(probclasslist[2],10)\n",
    "        word_log3= math.log(probclasslist[3],10)\n",
    "        \n",
    "        final_classscores_list.append(word_log0)\n",
    "        final_classscores_list.append(word_log1)\n",
    "        final_classscores_list.append(word_log2)\n",
    "        final_classscores_list.append(word_log3)\n",
    "        \n",
    "        Score_dict[word] = final_classscores_list\n",
    "    return Score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Implementation and test file generation\n",
    "\n",
    "def naive_bayes(file_name_writeread,file_name_write,main_dict,test_dict,Score_dict):\n",
    "    file_result = open(file_name_write, \"w\",encoding=\"utf-8\")\n",
    "    line_counter=0\n",
    "    predicted_class=[]\n",
    "    actual_class=[]\n",
    "    classes_list=[]   \n",
    "    classscores = []\n",
    "    class_prob_testlist=[]\n",
    "    for class_type in main_dict.keys():\n",
    "        class_prob_testlist.append(class_type)\n",
    "        class_score = prob_type_dict[class_type] # class score\n",
    "        word_log = math.log(class_score,10)\n",
    "        classscores.append(word_log)\n",
    "\n",
    "    for i, row in testdata.iterrows():\n",
    "        final_classscores_str=\"\"\n",
    "        final_classscores_list=[]\n",
    "        \n",
    "        line_counter=line_counter+1\n",
    "        result_string=\"\"\n",
    "        label_correct=testdata.at[i,'Post Type']\n",
    "        \n",
    "        word_log0 = classscores[0]\n",
    "        word_log1 = classscores[1]\n",
    "        word_log2 = classscores[2]\n",
    "        word_log3 = classscores[3]\n",
    "                \n",
    "        title_put=' '\n",
    "        unigram = []\n",
    "        nonunigram = []\n",
    "        vocab = []\n",
    "        pattern = '[\\w]+'\n",
    "        tokenizer = RegexpTokenizer(pattern)\n",
    "        title_put = row.Title\n",
    "        title = title_put.replace(\"'\",\"\")\n",
    "        list_wordtokenized=tokenizer.tokenize(title)\n",
    "        posttag_list = pos_tag(list_wordtokenized)\n",
    "        bigrams = list(nltk.bigrams(posttag_list))\n",
    "        for (w1,w2) in bigrams :       \n",
    "            flagw1 = False\n",
    "            flagw2 = False\n",
    "            flag_w1 = False\n",
    "            flag_w2 = False\n",
    "            posw1 = pos_mapping(w1[1])\n",
    "            posw2 = pos_mapping(w2[1])\n",
    "            lemword1=(lemmat.lemmatize(w1[0],posw1)).lower()\n",
    "            lemword2=(lemmat.lemmatize(w2[0],posw2)).lower()\n",
    "            \n",
    "            if('_' in lemword1):\n",
    "                word1 = lemword1.replace(\"_\",\"\")\n",
    "                if word1.isalpha():\n",
    "                    flag_w1 = True \n",
    "            if('_' in lemword2):\n",
    "                word2 = lemword2.replace(\"_\",\"\")\n",
    "                if word2.isalpha():\n",
    "                    flag_w2 = True \n",
    "            if (lemword1.isalpha()) or flag_w1:\n",
    "                flagw1 = True\n",
    "            else:\n",
    "                nonvocablist.append(lemword1) \n",
    "            if (lemword2.isalpha()) or flag_w2:    \n",
    "                flagw2 = True\n",
    "            else:\n",
    "                nonvocablist.append(lemword2)  \n",
    "            if posw1 == 'n' and posw2 == 'n' and flagw1 and flagw2:\n",
    "                x = \"%s %s\" % (lemword1.strip(),lemword2.strip())\n",
    "                unigram.append(x)\n",
    "                nonunigram.append(lemword1)\n",
    "                nonunigram.append(lemword2)      \n",
    "                if x in test_dict:\n",
    "                    val = Score_dict[x]\n",
    "                    word_log0=word_log0+val[0]\n",
    "                    word_log1=word_log1+val[1]\n",
    "                    word_log2=word_log2+val[2]\n",
    "                    word_log3=word_log3+val[3]\n",
    "                if lemword1 in unigram:\n",
    "                    unigram.remove(lemword1)\n",
    "                    if (lemword1 in test_dict):\n",
    "                        val = Score_dict[lemword1]\n",
    "                        word_log0=word_log0-(val[0])\n",
    "                        word_log1=word_log1-(val[1])\n",
    "                        word_log2=word_log2-(val[2])\n",
    "                        word_log3=word_log3-(val[3])\n",
    "                if lemword2 in unigram:\n",
    "                    unigram.remove(lemword2)\n",
    "                    if (lemword2 in test_dict):\n",
    "                        val = Score_dict[lemword2]\n",
    "                        word_log0=word_log0-(val[0])\n",
    "                        word_log1=word_log1-(val[1])\n",
    "                        word_log2=word_log2-(val[2])\n",
    "                        word_log3=word_log3-(val[3])\n",
    "            else:\n",
    "                if flagw1 and (lemword1 not in unigram) and (lemword1 not in nonunigram):\n",
    "                    unigram.append(lemword1)\n",
    "                    if lemword1 in test_dict:\n",
    "                        val = Score_dict[lemword1]\n",
    "                        word_log0=word_log0+val[0]\n",
    "                        word_log1=word_log1+val[1]\n",
    "                        word_log2=word_log2+val[2]\n",
    "                        word_log3=word_log3+val[3]\n",
    "                if flagw2 and (lemword2 not in unigram) and (lemword2 not in nonunigram):\n",
    "                    unigram.append(lemword2)\n",
    "                    if lemword2 in test_dict:\n",
    "                        val = Score_dict[lemword2]\n",
    "                        word_log0=word_log0+val[0]\n",
    "                        word_log1=word_log1+val[1]\n",
    "                        word_log2=word_log2+val[2]\n",
    "                        word_log3=word_log3+val[3]\n",
    "                   \n",
    "        \n",
    "\n",
    "        final_classscores_list.append(word_log0)\n",
    "        final_classscores_list.append(word_log1)\n",
    "        final_classscores_list.append(word_log2)\n",
    "        final_classscores_list.append(word_log3)\n",
    "        word_log0 = (\"{:12.10f}\".format(word_log0))\n",
    "        word_log1 = (\"{:12.10f}\".format(word_log1))\n",
    "        word_log2 = (\"{:12.10f}\".format(word_log2))\n",
    "        word_log3 = (\"{:12.10f}\".format(word_log3))\n",
    "        if final_classscores_str == \"\":\n",
    "            final_classscores_str = str(word_log0) + \"  \" + str(word_log1)+ \"  \" + str(word_log2)+ \"  \" + str(word_log3)\n",
    "\n",
    "            \n",
    "        max_probscore_index=final_classscores_list.index(max(final_classscores_list))\n",
    "        label_classified=class_prob_testlist[max_probscore_index]\n",
    "        rel=\"\"\n",
    "        if label_classified == label_correct:\n",
    "            rel=\"right\"\n",
    "        else:\n",
    "            rel=\"wrong\"\n",
    "        actual_class.append(label_correct)\n",
    "        predicted_class.append(label_classified)\n",
    "        result_string=result_string+str(line_counter)+\"  \"+str(title_put)+\"  \"+str(label_classified)+\"  \"+str(final_classscores_str)+\"  \"+str(label_correct)+\"  \"+rel\n",
    "        file_result.write(result_string + \"\\r\\n\")\n",
    "    file_result.close()\n",
    "    classes_list= np.unique(actual_class)\n",
    "    return (actual_class,predicted_class,classes_list)\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results analysis\n",
    "\n",
    "def result_analysis(actual_class,predicted_class,experiment,classes_list):\n",
    "    \n",
    "    \n",
    "    experiment=experiment.strip()\n",
    "    \n",
    "    if experiment=='exp1' or experiment=='exp2' or experiment=='exp3'or experiment=='exp4' or experiment=='exp5':\n",
    "        \n",
    "        print(\"experiment:-\",experiment)\n",
    "        print(\"\\n\")\n",
    "        print(\"----confusion matrix----\")\n",
    "        cm = pd.DataFrame(confusion_matrix(actual_class,predicted_class),columns=classes_list,index=classes_list)\n",
    "        print(cm)\n",
    "        print(\"\\n\")\n",
    "        print(\"----classification report----\")\n",
    "        print(classification_report(actual_class,predicted_class ,target_names=classes_list))\n",
    "        print(\"\\n\")\n",
    "        print(\"----Accuracy report----\")\n",
    "        print(accuracy_score(actual_class, predicted_class))\n",
    "\n",
    "    if experiment=='exp5' or experiment=='exp4' :\n",
    "        \n",
    "        accuracy_diffsmooth.append(accuracy_score(actual_class, predicted_class))\n",
    "   \n",
    "    #print(\"accuracy_diffsmooth\",accuracy_diffsmooth)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the experiment no.(exp1/exp2/exp3/exp4/exp5)exp5\n",
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          1     1        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.67      1.00      0.80         2\n",
      "        poll       1.00      0.33      0.50         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.85      0.83      0.79        10\n",
      "weighted avg       0.86      0.80      0.77        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.8\n",
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          2     0        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.50      1.00      0.67         2\n",
      "        poll       0.00      0.00      0.00         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.56      0.75      0.63        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.7\n",
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          2     0        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.50      1.00      0.67         2\n",
      "        poll       0.00      0.00      0.00         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.56      0.75      0.63        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.7\n",
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          2     0        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.50      1.00      0.67         2\n",
      "        poll       0.00      0.00      0.00         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.56      0.75      0.63        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.7\n",
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          2     0        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.50      1.00      0.67         2\n",
      "        poll       0.00      0.00      0.00         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.56      0.75      0.63        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.7\n",
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          2     0        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.50      1.00      0.67         2\n",
      "        poll       0.00      0.00      0.00         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.56      0.75      0.63        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.7\n",
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          2     0        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.50      1.00      0.67         2\n",
      "        poll       0.00      0.00      0.00         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.56      0.75      0.63        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.7\n",
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          2     0        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.50      1.00      0.67         2\n",
      "        poll       0.00      0.00      0.00         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.56      0.75      0.63        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshkour/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          2     0        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.50      1.00      0.67         2\n",
      "        poll       0.00      0.00      0.00         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.56      0.75      0.63        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.7\n",
      "experiment:- exp5\n",
      "\n",
      "\n",
      "----confusion matrix----\n",
      "         ask_hn  poll  show_hn  story\n",
      "ask_hn        2     0        0      0\n",
      "poll          2     0        0      1\n",
      "show_hn       0     0        2      0\n",
      "story         0     0        0      3\n",
      "\n",
      "\n",
      "----classification report----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.50      1.00      0.67         2\n",
      "        poll       0.00      0.00      0.00         3\n",
      "     show_hn       1.00      1.00      1.00         2\n",
      "       story       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.56      0.75      0.63        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n",
      "\n",
      "\n",
      "----Accuracy report----\n",
      "0.7\n",
      "Smooth Factor: \n",
      " [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
      "Accuracy: \n",
      " [0.8, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished\n"
     ]
    }
   ],
   "source": [
    "#Class for performing experiments for different vocab size and smoothing factor\n",
    "experiment= input(\"Enter the experiment no.(exp1/exp2/exp3/exp4/exp5)\").strip()\n",
    "smooth_factor=0.5\n",
    "#baseline experiment\n",
    "if experiment=='exp1':\n",
    "    \n",
    "    file_name_writeread=\"model-2018.txt\"\n",
    "    file_name_write=\"baseline-result.txt\"\n",
    "    \n",
    "    main_dict,test_dict= model_building(file_name_writeread,file_name_write,smooth_factor)\n",
    "    Score_dict = calcvocabscore(main_dict,test_dict)   \n",
    "    actual_class,predicted_class,classes_list=naive_bayes(file_name_writeread,file_name_write,main_dict,test_dict,Score_dict)\n",
    "    result_analysis(actual_class,predicted_class,experiment.strip(),classes_list)\n",
    "    \n",
    "    \n",
    "    print(\"Program finished\")\n",
    "#stopword removal    \n",
    "elif experiment=='exp2':\n",
    "    \n",
    "    stopwords_file=open(r\"Stopwords.txt\", \"r\",encoding=\"utf-8\")\n",
    "    file_cont=stopwords_file.read()\n",
    "    file_cont.replace(\"'\",\"\")\n",
    "    lines=file_cont.splitlines()\n",
    "    \n",
    "    for word in vocab_unique:\n",
    "        if word in lines:\n",
    "            vocab_unique.remove(word)\n",
    "        elif ' ' in word:\n",
    "            bigram = word.split(' ')\n",
    "            for b in bigram:\n",
    "                if (b in lines) and word in vocab_unique:\n",
    "                    vocab_unique.remove(word)       \n",
    "     \n",
    "     \n",
    "    vocab_unique.sort()\n",
    "    file_name_writeread=\"stopword-model.txt\"\n",
    "    file_name_write=\"stopword-result.txt\" \n",
    "    \n",
    "    main_dict,test_dict=model_building(file_name_writeread,file_name_write,smooth_factor)\n",
    "    Score_dict = calcvocabscore(main_dict,test_dict)\n",
    "    actual_class,predicted_class,classes_list=naive_bayes(file_name_writeread,file_name_write,main_dict,test_dict,Score_dict)\n",
    "    result_analysis(actual_class,predicted_class,experiment.strip(),classes_list)\n",
    "    \n",
    "    print(\"Program finished\")\n",
    "    \n",
    "#word-length filtering    \n",
    "elif experiment=='exp3':\n",
    "    \n",
    "    new_vocab=[]\n",
    "    for word in vocab_unique:\n",
    "        if (len(word)>=3 and len(word)<=8):\n",
    "            new_vocab.append(word)\n",
    "    vocab_unique=new_vocab \n",
    "    vocab_unique.sort()\n",
    "    \n",
    "    file_name_writeread=\"wordlength-model.txt\"\n",
    "    file_name_write=\"wordlength-result.txt\"\n",
    "    \n",
    "    main_dict,test_dict=model_building(file_name_writeread,file_name_write,smooth_factor)\n",
    "    Score_dict = calcvocabscore(main_dict,test_dict)\n",
    "    actual_class,predicted_class,classes_list=naive_bayes(file_name_writeread,file_name_write,main_dict,test_dict,Score_dict)\n",
    "    result_analysis(actual_class,predicted_class,experiment.strip(),classes_list)\n",
    "    \n",
    "    print(\"Program finished\")\n",
    "    \n",
    "#frequency filtering   \n",
    "elif experiment=='exp4':\n",
    "    new_vocab_len=[]\n",
    "    accuracy_diffsmooth=[]\n",
    "    total_freq_dict={} \n",
    "     \n",
    "    unique, counts = np.unique(vocablist, return_counts=True)\n",
    "    total_freq_dict = dict(zip(unique, counts))\n",
    "    \n",
    "    vocabset= set(total_freq_dict.keys())\n",
    "    \n",
    "    sorted(total_freq_dict.items(), key=lambda x: x[1])\n",
    "    \n",
    "    freq_removal=['1','l5','l10','l15','l20','p5','p10','p15','p20','p25']\n",
    "    less_flag=False\n",
    "    percent_flag=False\n",
    "    \n",
    "    for freq in freq_removal:\n",
    "        #print(\"Value is ---- \"+ str(freq))\n",
    "        \n",
    "        remove_words=[]\n",
    "        \n",
    "        if len(freq)>1:\n",
    "            if(freq[0]=='l'):\n",
    "                less_flag=True \n",
    "            else:\n",
    "                less_flag=False\n",
    "                percent_flag=True\n",
    "                \n",
    "            freq=int(freq[1:])\n",
    "        \n",
    "        if(percent_flag is True):\n",
    "            dict_len=len(total_freq_dict.items())\n",
    "            num_words=int((freq/100)*dict_len)\n",
    "            #print(num_words)\n",
    "            num_wordst = num_words\n",
    "            list2 = list(total_freq_dict.values())\n",
    "            valuelist, counts = np.unique(list2, return_counts=True)\n",
    "            valuedict = dict(zip(valuelist, counts))\n",
    "            valuetoberemoved = []\n",
    "            lastvalueremoved = 0\n",
    "            #print(list2)\n",
    "            while(num_words>0):\n",
    "                maxval = max(list2)\n",
    "                #print(\"maximum value is \" + str(maxval))\n",
    "                count = valuedict[maxval]\n",
    "                list2 = list(filter(lambda a: a != maxval, list2))\n",
    "                #print(\"count is \" + str(count))\n",
    "                num_words = num_words - count\n",
    "                #print(\"numwords is \" + str(num_words))\n",
    "                if num_words >= 0:\n",
    "                    valuetoberemoved.append(maxval)\n",
    "                else:\n",
    "                    lastvalueremoved = maxval\n",
    "                    #print(\"last value to be removed \" + str(lastvalueremoved))\n",
    "                #print(valuetoberemoved) \n",
    "                \n",
    "                \n",
    "            counting=0\n",
    "            \n",
    "            for word in total_freq_dict:\n",
    "                word_freq = total_freq_dict[word]\n",
    "                if(word_freq in valuetoberemoved) and (word in total_freq_dict) and counting<num_wordst:\n",
    "                    vocabset.discard(word)\n",
    "                    remove_words.append(word)\n",
    "                    counting=counting+1\n",
    "                if counting>=num_wordst:\n",
    "                    break\n",
    "            \n",
    "            #print(remove_words)\n",
    "            \n",
    "            if lastvalueremoved != 0:\n",
    "                for word in total_freq_dict:\n",
    "                    word_freq = total_freq_dict[word]\n",
    "                    if word_freq == lastvalueremoved:\n",
    "                        vocabset.discard(word)\n",
    "                        remove_words.append(word)\n",
    "                        counting=counting+1\n",
    "                        if counting>=num_wordst:\n",
    "                            break\n",
    "                      \n",
    "        else:   \n",
    "            if (less_flag is False) :\n",
    "                for word in total_freq_dict: \n",
    "                    word_freq = total_freq_dict[word]\n",
    "                    if(word_freq == int(freq)):\n",
    "                        if(word in total_freq_dict):\n",
    "                            vocabset.discard(word)\n",
    "                            remove_words.append(word)\n",
    "                      \n",
    "\n",
    "            elif (less_flag is True):\n",
    "                for word in total_freq_dict: \n",
    "                    word_freq = total_freq_dict[word]\n",
    "                    if(word_freq<=freq):\n",
    "                        if(word in total_freq_dict):\n",
    "                            vocabset.discard(word) \n",
    "                            remove_words.append(word) \n",
    "        \n",
    "        vocab_unique = list(vocabset - set(remove_words))\n",
    "        for wor in remove_words:\n",
    "            if wor in total_freq_dict:\n",
    "                del total_freq_dict[wor]\n",
    "        \n",
    "        #print(total_freq_dict)\n",
    "        vocab_unique.sort()\n",
    "        \n",
    "        new_vocab_len.append(len(vocab_unique))\n",
    "\n",
    "        file_name_writeread=\"model-2018.txt\"\n",
    "        file_name_write=\"baseline-result.txt\"\n",
    "\n",
    "        #print(\"modelling started\")\n",
    "        main_dict,test_dict=model_building(file_name_writeread,file_name_write,smooth_factor)\n",
    "        #print(\"modelling done\")\n",
    "        Score_dict = calcvocabscore(main_dict,test_dict)\n",
    "        #print(\"calc score done\")\n",
    "        actual_class,predicted_class,classes_list=naive_bayes(file_name_writeread,file_name_write,main_dict,test_dict,Score_dict)\n",
    "        #print(\"naivebyes done\")\n",
    "        result_analysis(actual_class,predicted_class,experiment.strip(),classes_list)\n",
    "        #print(\"result analysis done\")\n",
    "    \n",
    "    print(\"Remaining words in Vocab: \\n\",new_vocab_len)\n",
    "    print(\"Accuracy: \\n\",accuracy_diffsmooth)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(new_vocab_len, accuracy_diffsmooth,linestyle='--', marker='o', color='b') \n",
    "    plt.xlabel('Number of words remaining in vocab')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"EXPERIMENT 4 RESULTS\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Program finished\")\n",
    "    \n",
    "#changing smoothing factor    \n",
    "elif experiment=='exp5':\n",
    "    \n",
    "    accuracy_diffsmooth=[]\n",
    "    list_val=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "    \n",
    "    file_name_writeread=\"model-2018.txt\"\n",
    "    file_name_write=\"baseline-result.txt\"\n",
    "    \n",
    "    for i in list_val:\n",
    "        \n",
    "        smooth_factor=i\n",
    "    \n",
    "        main_dict,test_dict=model_building(file_name_writeread,file_name_write,smooth_factor)\n",
    "        Score_dict = calcvocabscore(main_dict,test_dict)\n",
    "        actual_class,predicted_class,classes_list=naive_bayes(file_name_writeread,file_name_write,main_dict,test_dict,Score_dict)\n",
    "        result_analysis(actual_class,predicted_class,experiment.strip(),classes_list)\n",
    "    \n",
    "    print(\"Smooth Factor: \\n\",list_val)\n",
    "    print(\"Accuracy: \\n\",accuracy_diffsmooth)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(list_val, accuracy_diffsmooth,linestyle='--', marker='o', color='b') \n",
    "    plt.xlabel('Smooth Factor')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"EXPERIMENT 5 RESULTS\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Program finished\")\n",
    "else:\n",
    "    print(\"Incorrect experiment no. has beeen entered\")\n",
    "    print(\"Program finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
